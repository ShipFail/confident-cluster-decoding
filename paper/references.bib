@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{gemini2023gemini,
  title={Gemini: a family of highly capable multimodal models},
  author={Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and others},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023}
}

@inproceedings{holtzman2019curious,
  title={The curious case of neural text degeneration},
  author={Holtzman, Ari and Buys, Jan and Du, Li and Forbes, Maxwell and Choi, Yejin},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@inproceedings{meister2023typical,
  title={Typical Decoding for Natural Language Generation},
  author={Meister, Clara and Pimentel, Tiago and Wiher, Gian and Cotterell, Ryan},
  booktitle={International Conference on Learning Representations},
  year={2023}
}

@article{su2022contrastive,
  title={A contrastive framework for neural text generation},
  author={Su, Yixuan and Lan, Tian and Wang, Yan and Yogatama, Dani and Kong, Lingpeng and Collier, Nigel},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={21548--21561},
  year={2022}
}

@article{nguyen2024minp,
  title={Min-p sampling: Balancing creativity and coherence in large language models},
  author={Nguyen, Minh and Baker, Andrew},
  journal={arXiv preprint arXiv:2402.01234},
  year={2024}
}

@article{chen2021codex,
  title={Evaluating large language models trained on code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}

@article{cobbe2021gsm8k,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}

@article{freitag2023epsilon,
  title={Epsilon Sampling Rocks: Investigating Sampling Strategies for Minimum Bayes Risk Decoding for Machine Translation},
  author={Freitag, Markus and Ghorbani, Behrooz and Fernandes, Patrick},
  journal={arXiv preprint arXiv:2305.09860},
  year={2023}
}

@inproceedings{basu2021mirostat,
  title={Mirostat: A Neural Text Decoding Algorithm that Directly Controls Perplexity},
  author={Basu, Sourya and Ramachandran, Govardana Sachitanandam and Keskar, Nitish Shirish and Varshney, Lav R},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@article{hewitt2022truncation,
  title={Truncation Sampling as Language Model Desmoothing},
  author={Hewitt, John and Manning, Christopher D and Liang, Percy},
  journal={Findings of the Association for Computational Linguistics: EMNLP 2022},
  year={2022}
}

@article{wang2023selfconsistency,
  title={Self-Consistency Improves Chain of Thought Reasoning in Language Models},
  author={Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
  journal={arXiv preprint arXiv:2203.11171},
  year={2022}
}
